{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3eaa1d72-be65-45d6-9277-8c1b4f42eb33",
   "metadata": {
    "editable": true,
    "panel-layout": {
     "height": 483.45001220703125,
     "visible": true,
     "width": 100
    },
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Large Language Models\n",
    "\n",
    "## **Session 4-1:** Introduction to LLMs\n",
    "\n",
    "<img src=\"../figures/LLM_sizes.png\" alt=\"fig\" width=\"300\" align=\"right\" style=\"padding: 30px;\" />\n",
    "\n",
    "Large Language Models (LLMs) are:  \n",
    "* Deep-learning-based models for natural language\n",
    "* _Large_ because:\n",
    "  * Model sizes: $10^9 .. 10^{12}$ Parameters\n",
    "  * Training copus: $10^{11} ... 10^{13}$ tokens\n",
    "  * Training _compute_: $10^{21} ... 10^{25}$ Floating Point Operations (FLOPS)\n",
    "* very resource hungry to train\n",
    "* Example $10^{22}$ FLOPS:\n",
    "    * on i7 7700K CPU with $241\\cdot 10{9}$ FLOPS/s: 1300 years  \n",
    "    * Nvidia RTX 4090 consumer GPU with $82\\cdot 10^{12}$ FLOPS: 3.9 years\n",
    "    * El Captain cluster (top cluster): $1.47 \\cdot 10^{18}$ FLOPS: 2h\n",
    "* __but note:__ this is only compute; data transfer, and memory size is a major bottleneck!\n",
    "* Training an LLM from scratch is in the order of the entire budget of a medium-sized university \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392953ea",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### The Transformer\n",
    "\n",
    "<img src=\"../figures/Transformer.png\" alt=\"fig\" width=\"300\" align=\"right\" style=\"padding: 30px;\" />\n",
    "\n",
    "\n",
    "* The modern language models (Llama, GPT, ...) are all based on the transformer architecture. \n",
    "    * GPT: generative pretrained __Transformer__\n",
    "* The _attention_ enables the model capture relationship of tokens/words throughout the input \n",
    "* _multi-headed attention_ refers to multiple (parallel) attention heads with seperate (learned) parameters\n",
    "    * different heads learn different relationships\n",
    "\n",
    "\n",
    "* The main parts are the encoder (left) and decoder (right). Some modern models as e.g. Phi-4 are decoder-only. \n",
    "\n",
    "* Note: see also the paper [Attention Is All You Need](https://doi.org/10.48550/arXiv.1706.03762).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- \n",
    "\n",
    "generative pretraining: first part of training with unlabeled text (predict next token)\n",
    "after first add and norm: residual connection \n",
    "decoder only: text  generation; encoder only: classifications and embedding but not as widely used\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6060a0e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Terms for LLMs: \n",
    "\n",
    "* __Token__: input and output of the model is not on the character level but token level.\n",
    "  * Tokenization is translating the input string into tokens and the output tokens into a string\n",
    "  * Typically, tokens are a mixture of words, sub-words, or single characters\n",
    "  * tokenizer \"vocabulary\" is model specific, see e.g. [here](https://huggingface.co/deepseek-ai/DeepSeek-R1/raw/main/tokenizer.json) at entry \"vocab\".\n",
    "  <!--* Rule of thumb: on average 100 tokens ~ 75 words-->\n",
    "* __context window__: The maximum number of tokens the model can consider for an output.  \n",
    "* __Model sizes__: 8B --> $8$ Billion parameters\n",
    "* __Quantization__: reduces the number of bits used to represent the model parameters\n",
    "    * is applied after training to reduce required memory and increase inference tokens\n",
    "* __Inference__: the process by which the (already trained) machine learning model obtains output from given input\n",
    "* __Multimodal__ model: models which support different types of input data, e.g. text, images, and audio.\n",
    "* __prompt__: The full input text given to the LLM.\n",
    "    * The prompt might include instructions, examples, and a task/question \n",
    "    * __query__: The specific task/question provided; part of the prompt\n",
    "    * __context__: Additional information included in the prompt to guide the model, such as examples, prior , conversations, or task-specific instructions\n",
    "  <!--* query: part of the prompt-->\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f57b0ba3-fea0-4ecc-a8e5-d9c3c980f911",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \"This is a simple sentence, showing tokenization at the Multibody Dynamics Workshop 2025 in Innsbruck. \"\n",
      "Tokens: ['This', 'is', 'a', 'simple', 'sentence', ',', 'showing', 'token', '##ization', 'at', 'the', 'Multi', '##body', 'Dynamics', 'Workshop', '202', '##5', 'in', 'Inn', '##s', '##bruck', '.']\n",
      "Input IDs: [1188, 1110, 170, 3014, 5650, 117, 4000, 22559, 2734, 1120, 1103, 18447, 14637, 25082, 16350, 17881, 1571, 1107, 9859, 1116, 27824, 119]\n"
     ]
    }
   ],
   "source": [
    "# Example Tokenization: \n",
    "from transformers import BertTokenizer\n",
    "# Initialize the tokenizer from Bert -> an older Language model from 2018. \n",
    "# On Hugging Face (provides transformers library) the tokenizer.json is provided together with the files: \n",
    "# https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct/blob/main/tokenizer.json for Qwen2.5\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Example sentence: multibody is not a single token but 2. \n",
    "sentence = \"This is a simple sentence, showing tokenization at the Multibody Dynamics Workshop 2025 in Innsbruck. \"\n",
    "#sentence = \"Welcome to my multibody dynamics talk. \"\n",
    "print(f'Sentence: \"{sentence}\"')\n",
    "\n",
    "# 1. Tokenization: Convert the sentence into tokens\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "print(f\"Tokens: {tokens}\")  # Output the tokens\n",
    "\n",
    "# 2. Convert tokens to input IDs (numerical representation)\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(f\"Input IDs: {input_ids}\") # Output the input_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdde3fe-b629-4760-8ceb-757390fc2249",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Prompt Engineering\n",
    "<!--span style=\"font-size: 16pt\"-->  \n",
    "\n",
    "How prompts are formulated has a major influence on the model’s output and its quality.\n",
    "* Simple prompt: \"Create a simulation model of a spring-damper system.\"  \n",
    "      vs.\n",
    "* detailed prompt: \"Consider a mass–spring-damper system with the following properties: mass m = 8 kg, stiffness k = 5000 N/m, and damping d = 50 Ns/m. The force applied to the mass is f = 100 N. Create a simulation model using Exudyn to simulate the dynamics of the mass–spring-damper system. The spring has a length of 5 cm and is relaxed in the initial position. Please write the code with no comments and in one block.\" \n",
    "* Creating _good_ prompts is typically heuristic\n",
    "* __System prompts__ (provided to the model by default) guide general behaviour, e.g:\n",
    "  * \"You are a helpful assistant. \"\n",
    "  * \"You are an AI assistant answering questions in the domain of mechanical engineering and an expert in multibody dynamics. \"\n",
    "\n",
    "These instructions heuristically improve the quality of responses. \n",
    "\n",
    "<!--/span-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99474866-3bac-4f4f-8938-7720cdaa4ad4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Training Corpus\n",
    "\n",
    "    \n",
    "LLMs are trained on a large amount of data\n",
    "* It is generally not known exactly on which data the model is trained.\n",
    "* GPT3.5 is trained on Wikipedia, Books, and \"Webcrawl\"\n",
    "* Model Performance depends strongly on data quality\n",
    "* Typically, GitHub is also part of the training corpus\n",
    "    * But: knowledge cut-off due to training date\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03012e3e-71b2-4a51-a364-c6f61b010498",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### How LLMs gain new knowledge: \n",
    "    \n",
    "1. Training from __scratch__ and include new data\n",
    "2. __Finetuning__: further training of (pre-trained) model\n",
    "    * __Catastrophic forgetting__ might happen: the LLM forgets previously known facts/tasks ´\n",
    "    * Parameter efficient fine tuning (PEFT): not the whole model is retrained, but only parts of it\n",
    "    * [Low Rank Adaptation (LORA)](https://arxiv.org/abs/2106.09685): initial model parameters are frozen and additional _adapters_ (low-rank matrices) are added to the model and trained \n",
    "4. Retrievel Augmented Generation (__RAG__):\n",
    "    * A __database__ is provided: e.g. as pdf files from books, publications or code documentation \n",
    "    * A __retriever__ uses either keyword matching or neural embeddings to find semantically similar documents and returns k most relevant documents\n",
    "    * The __prompt__ is processed together with retrieved documents\n",
    "\n",
    "5. In-Context-Learning:\n",
    "    * LLMs learn during inference  by using examples or instructions previously provided through the input without updating the model's weights\n",
    "    * No explicit _training_ is performed\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e2b13de0-8c7e-4822-945d-6e88aebd023d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Availible Models\n",
    "\n",
    "__Disclaimer__: LLMs are a fast-paced topic with new libraries and models coming up often. \n",
    "__Note__: Most companies develop both large flagship models to push SOTA and smaller, more efficient models\n",
    "\n",
    "Proprietory:   \n",
    "* OPEN AI: GPT4o, GPT-4.1, GPT-4.1-mini, o1; see also the [model list](http://platform.openai.com/docs/models). \n",
    "* Google: Gemini 2.0 Flash, 2.5 Pro\n",
    "* Anthropic: Claude 3.5 Haiku, Claude 3.7 Sonnet\n",
    "\n",
    "Open (weights):  \n",
    "* Meta: Llama Models\n",
    "  * [Llama 2](https://www.llama.com/llama2/)/[Llama 3](https://www.llama.com/models/llama-3/): 8B, 70B, 405B in sizes of 8B, 70B, 405B\n",
    "  * [Llama 4](https://ai.meta.com/blog/llama-4-multimodal-intelligence/): mixture of expert models with up to 10M context length;  _Scout_: 109B/17B active parameters\n",
    "  * Note: for Llama models, an authorization request is required\n",
    "* Microsoft:\n",
    "    *   [Phi-4](https://huggingface.co/microsoft/phi-4), and Phi-3.   \n",
    "* Deepseek:\n",
    "  * [Deepseek-R1](https://arxiv.org/abs/2501.12948): strong reasoning Model\n",
    "  * [DeepSeek-Coder](https://arxiv.org/abs/2401.14196)\n",
    "* Mistral AI:\n",
    "  * Mistral\n",
    "  * Codestral\n",
    "* Alibaba AI:\n",
    "  * Qwen2.5 32B/72B, Qwen-Coder\n",
    " \n",
    "For many models - even proprietary ones - technical report _papers_ are available. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46ebdd3-d921-4372-8daf-310859dbd748",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Quantization: \n",
    "\n",
    "\n",
    "Besides compute, the available memory limits the speed of LLM inference. \n",
    "Inference performance of LLMs often only degrades slightly by using parameters down to 4-bit precision see e.g. [Dettmers, Zettlemoyer: the case for 4-bit precision: k-bit Inference Scaling Laws, 2023](https://doi.org/10.48550/arXiv.2212.09720). \n",
    "\n",
    "For Llama3, 70B, the model size in the memory is:  \n",
    "* $70*4 = 280GB$ in 32-bit floating point precision (also called fp32 or single-precision)\n",
    "* $70*0.5 = 35GB$ in 4bit precision\n",
    "  * this is denoted as Llama3 70B __Q4__\n",
    "\n",
    "Modern GPU hardware often supports larger FLOPS/s for lower precision, resulting in higher throughput.   \n",
    "__Note__: A 70B-Q4 model requires 35GB in memory, while an 8B model in fp32 is 32GB. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed07f173-08d8-46b1-901a-a299789e2902",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Libraries: \n",
    "\n",
    "\n",
    "__transformers__:  \n",
    "Provided by _Hugging Face_, transformers is a Python library of pretrained natural language processing, computer vision, audio, and multimodal models\n",
    "\n",
    "__gpt4all__:  \n",
    "Provides a python library and graphics interface. Includes simple indexing of local documents with RAG. \n",
    "\n",
    "__llamacpp__:  \n",
    "An efficient C++ library for LLM inference, which also includes a Python interface.  \n",
    "_Note_: although installable with <code> pip install llama-cpp-python </code> a C++ compiler is required for installation. \n",
    "\n",
    "\n",
    "__bitsandbytes__:  \n",
    "Provides local quantization, used e.g. in transformers. \n",
    "\n",
    "__torch__ and __tensorflow__ are general ML libraries typically running in the backend. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e24d53-d072-4166-bcdc-4c7182252399",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Session Content: \n",
    "\n",
    "* [4-2_Inspect_Local_Model](4-2_Inspect_Local_Model.ipynb): Load a local model and inspect its properties. \n",
    "* [4-3_CreateOscillator](4-3_CreateOscillator.ipynb): Create the simulation code for a simple Oscillator using a local model.  \n",
    "* [4-4_LLM_API](4-4_LLM_API.ipynb): Create a multibody model of a Slider-Crank using context learning and the (commercial) OpenAI online API. \n",
    "* [4-5_Optional_CreateMultibodyModel](4-5_Optional_CreateMultibodyModel.ipynb):  Create a Slider-Crank model using the open-source code Exudyn using a local model. Disclaimer: This requires more resources and parts use llama_cpp, which is only for demonstration. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "panel-cell-order": [
   "3eaa1d72-be65-45d6-9277-8c1b4f42eb33"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
