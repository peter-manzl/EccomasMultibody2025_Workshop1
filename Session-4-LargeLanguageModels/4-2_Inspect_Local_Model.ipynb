{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5ddd8a5-9613-4368-a6f8-311b67aebd62",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Getting Started with Local Large Language Models\n",
    "\n",
    "In this file, a local language model will be applied to create models of small-scale multibody problems. \n",
    "\n",
    "First, the libraries are loaded:  \n",
    "* gpt4all: used to run models locally; can be used both as a GUI and a Python library\n",
    "* transformers: library for pre-trained models for inference applications and fine-tuning\n",
    "* Hugging Face Hub: is a git-based repository, offering a wide range of pre-trained models\n",
    "\n",
    "If enough VRAM (GPU) or RAM (CPU) is available, __Phi-4__ can be used. The 4-bit quantized model (Q4) requires 8 GB of memory. In case your computer does not have enough memory __Phi-3__ can also be used - although it is not as capable - by setting  \n",
    "__<code>flagSmall = True </code>__  \n",
    "in the next code block. Note that the respective model is automatically downloaded when running this script. You can technically run models that exceed the available memory by swapping to the disk, but this will slow them down considerably. \n",
    "\n",
    "Task: \n",
    "* Run script locally\n",
    "* Optional: browse models on [huggingface](https://huggingface.co/) and find a model you want to run\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "617521fd-72c3-4616-8f7f-8738b98868e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "from transformers import AutoConfig\n",
    "from huggingface_hub import hf_hub_download\n",
    "from LLMHelperFunctions import CheckOutputLLM # helper function\n",
    "import torch # only used to check if cuda is available\n",
    "import time\n",
    "\n",
    "flagSmall = False\n",
    "\n",
    "if flagSmall: \n",
    "    # approx 2.4GB\n",
    "    repo_id = 'microsoft/phi-3-mini-4k-instruct-gguf'\n",
    "    filename =  \"Phi-3-mini-4k-instruct-q4.gguf\"\n",
    "else: \n",
    "    repo_id = \"bartowski/phi-4-gguf\"\n",
    "    filename = \"phi-4-Q4_K_S.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75264a2-f936-4b5c-a208-671457c58da3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Inspect model\n",
    "\n",
    "We can now inspect the model configuration from the repository. Note that, depending on the model, there might be several versions in the repository available, see e.g. [phi-4](https://huggingface.co/microsoft/phi-4). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21815cdb-2de1-43be-8d09-d8cd47faca85",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "repo id:  bartowski/phi-4-gguf BartConfig {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"bart\",\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": false,\n",
      "  \"transformers_version\": \"4.53.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(repo_id)\n",
    "print('repo id: ', repo_id, config, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897b9e3-a134-43c8-b4b5-fd72adead90b",
   "metadata": {
    "editable": true,
    "raw_mimetype": "",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Load Model\n",
    "\n",
    "\n",
    "The model is automatically downloaded and then loaded either on the GPU ('cuda') if available or into the RAM for running inference on the CPU.  \n",
    "By default, the model location is:  \n",
    "<!-- <code> `C:\\Users\\<username>\\.cache\\huggingface\\` </code> -->\n",
    "<code> `%USERPROFILE%\\.cache\\huggingface\\hub` </code>\n",
    "\n",
    "__Note__: If you are short on memory on your PC, you should later delete the LLM file model manually as it will be not deleted with the Python environment and the models are multiple GBs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ddd9e2d-7639-4afb-b1ad-fadc1414473e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model phi-4-Q4_K_S.gguf downloaded to local directory: \n",
      "C:\\Users\\C8501100\\.cache\\huggingface\\hub\\models--bartowski--phi-4-gguf\\snapshots\\19cd65f97c2f1712a81c506611d3f9c94b16a1e1\\phi-4-Q4_K_S.gguf\n",
      "\n",
      "running LLM on GPU\n"
     ]
    }
   ],
   "source": [
    "modelPath = hf_hub_download(repo_id=repo_id, filename=filename)\n",
    "print('model {} downloaded to local directory: \\n{}\\n'.format(filename, modelPath))\n",
    "try: \n",
    "    model = GPT4All(modelPath, device='cuda')\n",
    "    print('running LLM on GPU')\n",
    "except: \n",
    "    model = GPT4All(modelPath, device='cpu')\n",
    "    print('running LLM on CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7a6bb2f-1b63-4726-a014-2e92795dab20",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many eigenmodes does a two-mass spring-damper have?\n",
      "inference took 4.39s\n",
      "**\n",
      "\n",
      "To determine the number of eigenmodes in a system, we need to consider its degrees of freedom. A two-mass spring-damper system typically consists of:\n",
      "\n",
      "- Two masses\n",
      "- Springs connecting these masses and possibly fixed points (e.g., walls)\n",
      "- Dampers providing damping forces\n",
      "\n",
      "Each mass can move independently along one dimension (assuming motion is constrained in a single direction, such as horizontally). Therefore, the system has two degrees of freedom.\n",
      "\n",
      "The number of eigenmodes corresponds to the number of independent ways the system can oscillate. For each degree of freedom, there is typically an associated mode shape and natural frequency. Thus, for this two-mass spring-damper system with two degrees of freedom, it will have **two eigenmodes**.\n",
      "\n",
      "These modes describe how the masses move relative to one another when the system vibrates naturally (without external forcing). Each mode has a specific pattern of motion and associated natural frequency.\n"
     ]
    }
   ],
   "source": [
    "strQuestion = \"How many eigenmodes does a two-mass spring-damper have?\"\n",
    "print(strQuestion)\n",
    "t1 = time.time()\n",
    "output = model.generate(strQuestion, max_tokens=int(1e3))\n",
    "dt = time.time() - t1\n",
    "print(f'inference took {round(dt, 2)}s')\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab49298e-c87f-453e-b513-30b982a9782c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Output\n",
    "\n",
    "The model is trained to predict the next tokens, but does not neccesarily stop after answering the question. There are special tokens to help structure inputs and outputs, see [4-3_CreateOscillator](4-3_CreateOscillator.ipynb). \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
