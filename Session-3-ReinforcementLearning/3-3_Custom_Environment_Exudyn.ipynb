{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48a62469-8dea-4a38-9c8d-d2a5b05d0323",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#  Reinforcement Learning (RL)\n",
    "\n",
    "## **Session 3-3:** Custom environment using Multibody Simulation Code Exudyn \n",
    "\n",
    "This example is based on the Exudyn example [OpenAIgymNLinkAdvanced](https://github.com/jgerstmayr/EXUDYN/blob/master/main/pythonDev/Examples/openAIgymNLinkAdvanced.py). Is uses the open-source multibody dynamics code Exudyn for simulating in a _custom_ environment which enables the use of many simulation options including friction models or flexible bodies.  \n",
    "\n",
    "Authors: Johannes Gerstmayr and Peter Manzl; see also [License](https://github.com/jgerstmayr/EXUDYN/blob/master/LICENSE.txt). \n",
    "\n",
    "Training can be visualized using tensorboard: \n",
    "* open anaconda prompt (or bash with your Python environment)\n",
    "* go to current directory / solution\n",
    "* then call tensorboard as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb23518-d34a-4cac-abf7-5ae4cda996a1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorboard --logdir=D:\\Work_Uni\\WorkshopMLEccomas2025\\Session-3-ReinforcementLearning/tensorboard_log\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "strTensorboard = 'tensorboard --logdir={}/tensorboard_log'.format(os.getcwd())\n",
    "print(strTensorboard) # run code to get your current directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cbb54-a441-473a-8409-f00c0ca19207",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "then open [http://localhost:6006/](http://localhost:6006/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a34d1f56-1137-4cc7-a912-06430593a926",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from exudyn.artificialIntelligence import OpenAIGymInterfaceEnv, spaces, logger\n",
    "import numpy as np\n",
    "import sys\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import stable_baselines3\n",
    "\n",
    "from stable_baselines3.common.logger import Logger # logger for reward\n",
    "from environmentExudyn import InvertedNPendulumEnv, RewardLoggingCallback, make_env_custom\n",
    "import tensorboard\n",
    "from datetime import datetime\n",
    "flagTensorboard = True  # set to False if you want to print output here instead \n",
    "\n",
    "if __name__ == '__main__': \n",
    "    # this is only executed when file is direct called in Python; required for multiprocessing. \n",
    "    import time\n",
    "    #use some learning algorithm:\n",
    "    from stable_baselines3 import A2C, SAC, PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077f81d-6f5b-49af-96c7-fec00ccce0c1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Tensorboard and Logging\n",
    "    \n",
    "Tensorboard provides us with live visualization of the training process, e.g. mean reward, cumulative reward per episode, and more. Custom information can also be added, depending on the environment. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676de7e0-b7a2-493a-9743-0a3e82cb8ee3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Create Environment\n",
    "\n",
    "The environment, implemented in the [environmentExudyn](./environmentExudyn.py), is what the agent is interacting with, in our case the single-link inverted pendulum on the cart. The environment is the MBS simulation model and is created as an object, derived from the OpenAIGymInterfaceEnv from Exudyn. The following functions are required: \n",
    "* __step__: Defines how a timestep is performed. The agent's action is mapped to the MBS, the solver is called to integrate one or more timestep(s), check if episode ended, calculate reward and write logging information.\n",
    "    * _MapAction2MBS_: Defines how the action influences the MBS, e.g. applies a force or changes a setpoint for control.\n",
    "    * _IntegrateStep_: Calls solver of MBS. \n",
    "    * _Output2StateAndDone_: Calculates observation from MBS system state and checks if episode ends (\"done\"). \n",
    "    * _getReward_: Calculates the agent's current reward \n",
    "* __reset__: After each episode, the system is reset, initializing the MBS in a new (random) system state. \n",
    "* __CreateMBS__: The multibody system is defined and created in this Exudyn-specific function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e354a8c-efde-4160-9305-385c7297fe3c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Create Agent\n",
    "\n",
    "In this example, the __getModel__ function provides us with different RL methods for the agents: \n",
    "* __SAC__: Soft Actor Critic\n",
    "* __A2C__: Advantage Actor Critic \n",
    "* __PPO__: Proximal Policy Optimization  \n",
    "\n",
    "Note that the batch_size in SAC is a major factor for the computation time per step as SAC is off-policy and uses a buffer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae4a95b-3ec6-4dce-bd4e-70db1f697345",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output written to tensorboard, start tensorboard to see progress!\n"
     ]
    }
   ],
   "source": [
    "    modelType = 'A2C' # use e.g. A2C, SAC or PPO with predefined parameters\n",
    "    tensorboard_log = None #no logging\n",
    "    rewardCallback = None\n",
    "    verbose = 0 #turn off to just view in tensorboard\n",
    "    if flagTensorboard: #only us if tensorboard is available\n",
    "        tensorboard_log = \"tensorboard_log/\" #dir\n",
    "        rewardCallback = RewardLoggingCallback()\n",
    "        print('output written to tensorboard, start tensorboard to see progress!')\n",
    "    else:\n",
    "        verbose = 1 #turn on without tensorboard\n",
    "    \n",
    "    # here the model is loaded (either for vectorized or scalar environment using SAC, A2C or PPO).     \n",
    "    def getModel(myEnv, modelType='SAC'): \n",
    "        if modelType=='SAC': \n",
    "            model = SAC('MlpPolicy',\n",
    "                   env=myEnv,\n",
    "                   learning_rate=1e-3,\n",
    "                   device='cpu', #usually cpu is faster for this size of networks\n",
    "                   batch_size=128,\n",
    "                   buffer_size=int(10e3),\n",
    "                   tensorboard_log=tensorboard_log,\n",
    "                   verbose=verbose)\n",
    "        elif modelType == 'A2C': \n",
    "            model = A2C('MlpPolicy', \n",
    "                    myEnv, \n",
    "                    device='cpu',  \n",
    "                    tensorboard_log=tensorboard_log,\n",
    "                    verbose=verbose)\n",
    "        elif modelType == 'PPO': \n",
    "            model = PPO('MlpPolicy', \n",
    "                    myEnv, \n",
    "                    device='cpu',  \n",
    "                    tensorboard_log=tensorboard_log,\n",
    "                    verbose=verbose)\n",
    "        else: \n",
    "            print('Please specify the modelType.')\n",
    "            raise ValueError\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4499b22e-043c-4be6-9d83-128b1428a163",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Start training\n",
    "    \n",
    "Note that with the implemented  ```flagParallel = True```, _vectorized_ environments are used. Multiple environments run in parallel on different threads/CPU-cores for data generation. Training times strongly depend on the used algorithms and settings and due to race conditions results might not be deterministic - even when setting the seed.  \n",
    "The value of ```flagContinuous``` can be used to change between continuous and discrete action space (\"bang-bang control\"). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fc3b47c-571d-434a-809c-0f27594714dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using 12 cores\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8d53abed4542d39a082c3b825e6f43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start learning of agent with ActorCriticPolicy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** learning time total = 95.90342783927917 ***\n"
     ]
    }
   ],
   "source": [
    "    modelName = '{}_cartPole_{}'.format(modelType, datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\"))\n",
    "    nSteps = 250e3\n",
    "    flagContinuous = False\n",
    "    flagParallel = True\n",
    "    if not(flagParallel): #'scalar' environment, slower because it uses only one CPU:\n",
    "        env = InvertedNPendulumEnv(nLinks=1, flagContinuous=flagContinuous)\n",
    "        # env.TestModel(numberOfSteps=2000, seed=42, sleepTime=0.02, useRenderer=True) # visualize environment by running 2000 steps\n",
    "\n",
    "        model = getModel(env, modelType=modelType) \n",
    "        ts = -time.time()\n",
    "        print('start learning ', modelName)\n",
    "        model.learn(total_timesteps=int(nSteps), #min 250k steps required to start having success to stabilize double pendulum\n",
    "                    # progress_bar=True, #requires tqdm and rich package; set True to only see progress and set log_interval very high\n",
    "                    log_interval=1, #logs per episode; influences local output and tensorboard\n",
    "                    callback = rewardCallback,\n",
    "                    )\n",
    "        \n",
    "        print('*** learning time total =',ts+time.time(),'***')\n",
    "    \n",
    "        #save learned model\n",
    "        \n",
    "        model.save(\"solution/\" + modelName)\n",
    "   \n",
    "    else: #parallel; faster #set verbose=0 in getModel()!\n",
    "        import torch #stable-baselines3 is based on pytorch\n",
    "        n_cores= max(1,int(os.cpu_count()*1.5//2)) # n_cores is  number of real cores (not threads)\n",
    "        torch.set_num_threads(n_cores) #seems to be ideal to match the size of subprocVecEnv\n",
    "        \n",
    "        print('using',n_cores,'cores')\n",
    "\n",
    "        from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "        vecEnv = SubprocVecEnv([make_env_custom(flagContinuous=flagContinuous) for i in range(n_cores)])\n",
    "        \n",
    "        #main learning task\n",
    "        model = getModel(vecEnv, modelType=modelType)\n",
    "\n",
    "        ts = -time.time()\n",
    "        print('start learning of agent with {}'.format(str(model.policy).split('(')[0]))\n",
    "\n",
    "        model.learn(total_timesteps=int(nSteps),\n",
    "                    progress_bar=True, #requires tqdm and rich package; set True to only see progress\n",
    "                    log_interval=1, # logs per episode; influences local output and tensorboard\n",
    "                    callback = rewardCallback,\n",
    "                    )\n",
    "        print('*** learning time total =',ts+time.time(),'***')\n",
    "    \n",
    "        #save learned model\n",
    "        model.save(\"solution/\" + modelName)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e51d0-ab09-4236-8a9d-63e1d62a2ece",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Test Agent\n",
    "\n",
    "The trained agent is tested in a environment with larger threasholds and visualized. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f8e5dfd-5031-49c8-aacb-b2127e659a92",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time spent= 0.8140072822570801\n",
      "columns imported = [2, 2, 2, 0, 0, 0, 0]\n",
      "total columns to be imported = 6 , array size of file = 7\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__': #  load and test\n",
    "    if modelType == 'A2C': \n",
    "        model = A2C.load(\"solution/\" + modelName)\n",
    "    elif modelType == 'SAC': \n",
    "        model = SAC.load(\"solution/\" + modelName)\n",
    "    elif modelType == 'PPO':\n",
    "        model = PPO.load(\"solution/\" + modelName)\n",
    "        \n",
    "    env = InvertedNPendulumEnv(thresholdFactor=5, nLinks=1, flagContinuous=flagContinuous) #larger threshold for testing\n",
    "    \n",
    "    solutionFile='solution/learningCoordinates.txt'\n",
    "    env.TestModel(numberOfSteps=1000, model=model, solutionFileName=solutionFile, \n",
    "                  stopIfDone=False, useRenderer=False, sleepTime=0) #just compute solution file\n",
    "\n",
    "    #visualize (and make animations) in exudyn:\n",
    "    from exudyn.interactive import SolutionViewer\n",
    "    from exudyn.utilities import LoadSolutionFile\n",
    "    \n",
    "    env.SC.visualizationSettings.general.autoFitScene = False\n",
    "    solution = LoadSolutionFile(solutionFile)\n",
    "    \n",
    "    SolutionViewer(env.mbs, solution, timeout=0.005, rowIncrement=2) #loads solution file via name stored in mbs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
