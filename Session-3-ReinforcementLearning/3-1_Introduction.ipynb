{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5402e6-6498-4ffc-9da9-7f5458b38868",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "#  Reinforcement Learning (RL)\n",
    "\n",
    "## **Session 3-1:** Introduction to Reinforcement Learning\n",
    "\n",
    "In this module, the basics of reinforcement learning will be shown. You will learn the required components for setting up the agent and learning environment and how the critical components interact. \n",
    "\n",
    "Reinforcement Learning is a (deep) learning method where: \n",
    "* the data is created by the interaction of an agent with the environment in the learning process.   \n",
    "* the learning process is driven by the _reward_ - a metric calculated from the environment \n",
    "    * the objective can be directly formulated\n",
    "    * no knowledge required of what good behaviour is\n",
    "* __Challenge__: data depends on learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2c9db0-ff46-4a9d-afbd-d62cb90dc93e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### A (very) Brief History of RL\n",
    "\n",
    "<!--span style=\"font-size: 16pt\"-->  \n",
    "  \n",
    "* __1983__: [Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems](doi.org/10.1109/TSMC.1983.6313077) introduces concept of _Actor-Critic_\n",
    "* __1989__: Q-learning is introduced by Watkins\n",
    "* __2013__: Deep Q-Network (DQN) play Atari games - and beat human performance in 2015\n",
    "* __2015/2016__: AlphaGo beats human masters in Go, which was previously thought too complicated for AI.  \n",
    "* __2017__: Proximal policy optimization (PPO) is published\n",
    "* __2022__: Reinforcement Learning Through Human Feedback (RLHF) is used in InstructGPT for fine-tuning Language models - which is today standard. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b50d236-2bb8-4841-88d7-700ddc6c4182",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Components: \n",
    "<img src=\"../figures/RLScheme.png\" alt=\"fig\" width=\"450\" align=\"right\" style=\"padding: 30px;\" />\n",
    "\n",
    "\n",
    "* __agent__:  \n",
    "    * in (deep) RL the agent consists of one or more neural networks\n",
    "    * The network parameters are adjusted in the learning process \n",
    "    * __policy__: the strategy/behaviour of the agent\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* __environment__: refers to the system or context within which the agent operates  \n",
    "    * simulation model, e.g. multibody dynamics model  \n",
    "    * real system --> might be safety-critical  \n",
    "    * The environment is advanced in _steps_  \n",
    "        * In every environment _step_ the agent recieves current __reward__, __state__ and  and returns the __action__   \n",
    "        * A single step in the environment might correspond to multiple steps in the simulation model  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d0450c-35a1-42f7-bb85-58bdd78818e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Components: \n",
    "<img src=\"../figures/RLScheme.png\" alt=\"fig\" width=\"450\" align=\"right\" style=\"padding: 30px;\" />\n",
    "\n",
    "\n",
    "\n",
    "* __action__: how the agent interacts with the environment:  \n",
    "    * could be a setpoint for PD-control, a force, torque, ...  \n",
    "    * either discrete or continuous depending on agent's algorithm\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* __state__ (observation): \n",
    "    * The data from the environment observed by the agent \n",
    "    * In a _real_ system this could be sensor data\n",
    "    * action is chosen based on current state\n",
    "    * An observation is a partial description of a state, often the state is used synonymously \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d4421a-32cc-48f6-90e8-7172dd2051b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Components: \n",
    "<img src=\"../figures/RLScheme.png\" alt=\"fig\" width=\"450\" align=\"right\" style=\"padding: 30px;\" />\n",
    "\n",
    "\n",
    "* __reward__:\n",
    "    * Used to learn from the environment   \n",
    "    * essential design choice\n",
    "    * Policy is trained by the optimizer to maximize the expected cumulative reward\n",
    "    * determines good/bad behaviour by giving high/low reward\n",
    "\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* __episodes__:\n",
    "    * training is organised in episodes, where one episode is a sequence of subsequent interactions of the agent with the environment\n",
    "    * at the start of each episode the system is _reset_, thus randomly initialized\n",
    "    * the end of an episode is reached when: \n",
    "        * truncated, e.g. time is over or agent made no progress for longer time\n",
    "        * terminated; environment's state/observation left permitted range, e.g. pendulum fell over\n",
    "        * terminated and truncated might not be distinguished in algorithm\n",
    "    * Typically, the range of values is limited for each state\n",
    "        * if the state is outside the permitted range environment is terminated\n",
    "        * This helps to learn efficiently and avoid wasting resources on failed episodes\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* __value__:\n",
    "    * how good is it to be in a state?\n",
    "        * generally depends on state and policy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cae76bc-1053-47e2-877d-348973eeb647",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example 1: _cartpole_ \n",
    "\n",
    "\n",
    "<img src=\"../figures/cartpole.png\" alt=\"fig\" width=\"450\" align=\"right\" style=\"padding: 30px;\"/>\n",
    "\n",
    "* inverted pendulum on a linear actuator\n",
    "* environment: multibody model, either using:\n",
    "    * redundant coordinates of bodies $[x_1, y_1, \\varphi_1, x_2, y_2, \\varphi_2]$, 1 prismatic, and 1 revolute joint\n",
    "    * minimal coordinates $[q_1, q_2] = [x_\\mathrm{cart}, \\varphi]$\n",
    "* state: [$x_{cart}$, $\\dot{x}_{cart}$, $phi$, $\\dot{\\varphi}$]\n",
    "    * state is obtained from MBD-model  \n",
    "    * also redundant coordinates could be used  \n",
    "* action: Force on cart $F_\\mathrm{cart}$, continuous or discrete (depending on agent!)\n",
    "* see [example training](./02_Exudyn_ExamplePendulum.ipynb) and [implementation environment](./environmentExudyn.py) for a custom environment using Exudyn.\n",
    "* reward e.g. $r = 1 - \\frac{\\varphi}{\\varphi_{max}}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30c82b6-2d35-40f9-a124-289a734864e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Gym / Gymnasium Interface\n",
    "<img src=\"../figures/cartpole.png\" alt=\"fig\" width=\"450\" align=\"right\" style=\"padding: 30px;\"/>\n",
    "  \n",
    "\n",
    "Gymnasium, formerly developed as OpenAI Gym, is a library for RL environments; most libraries ether support it or are compatible with it. The cartpole is a standard example - see also [here](https://gymnasium.farama.org/environments/classic_control/cart_pole/). \n",
    "At default: \n",
    "* The reward $r=1$ is given constantly as long as it does not fall over. \n",
    "* At reset, all states at initialization randomly from interval $(-0.05, 0.05)$\n",
    "* The episode ends when $|\\varphi| > 12Â°$, $|x_{cart}| > 2.4$m or episode length $>500$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "980afa35-ea5b-404e-ae90-9fd34e0c6e7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation at initialization \n",
      "[x, x_t, phi, phi_t] =  [ 0.0273956  -0.00611216  0.03585979  0.0197368 ]\n",
      "action:  0\n",
      "new observation:  [ 0.02727336 -0.20172954  0.03625453  0.32351476] \n",
      "\n",
      "reward = 1.0, terminated=False, truncated=False, info={}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\") # create environment object\n",
    "\n",
    "# when resetting with the seed we always get the same initialization\n",
    "observation, info = env.reset(seed=42) \n",
    "print('observation at initialization \\n[x, x_t, phi, phi_t] = ', observation) \n",
    "\n",
    "for _ in range(1): # do a single step\n",
    "    action = env.action_space.sample() # here a random action is chosen from {0,1} --> \n",
    "    observation, reward, terminated, truncated, info = env.step(action) # apply action (force) and call solver\n",
    "\n",
    "    if terminated or truncated:\n",
    "        observation, info = env.reset() # if done (truncatd/terminated) environment is reset \n",
    "\n",
    "print('action: ', action)\n",
    "print('new observation: ', observation, '\\n')\n",
    "print('reward = {}, terminated={}, truncated={}, info={}'.format(reward, terminated, truncated, info))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b451c5-e9f3-4180-b0d1-63ba5f3a10c7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Example 2: Bicycle\n",
    "\n",
    "    \n",
    "* environment:\n",
    "    * multibody model state $\\mathbf{s}_\\mathrm{MBD}$ and path $\\mathbf{s}_\\mathrm{path}$\n",
    "    * $\\mathbf{s}_\\mathrm{MBD} = [x_P, y_P, \\Psi, \\varphi, \\delta, \\theta_R, \\theta_F, \\dot{\\varphi}, \\dot{\\delta}, \\dot{\\theta_f}]$\n",
    "    *  $\\mathbf{s}_\\mathrm{path}$ contains lateral distance to path and preview points \n",
    "* action: setpoint for desired steering angle $\\delta$ for underlying PD-control\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "<img src=\"../figures/bike.png\" alt=\"fig\" width=\"300\"/>\n",
    "<img src=\"../figures/bike-preview.png\" alt=\"fig\" width=\"450\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28420145-986f-4a38-8347-9d62d350b680",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Challenges in RL: \n",
    "* explorations vs. exploitation:  \n",
    "    * _exploration_: trying new or less-visited actions to discover potentially better outcomes.\n",
    "    * Too much exploration might waste resources and converge slowly. \n",
    "    * _exploitation_: choosing the best known action based on the current knowledge.\n",
    "    * Too much exploitation might lead to suboptimal behaviour (local optima).\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* data efficiency:  \n",
    "    * describes how many _interactions_ with the environment are needed for learning\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* _Sensitivity_ to hyperparameters:\n",
    "    * Compared to supervised learning, there is a feedback loop of the agent to the acquired data over the policy\n",
    "    * This increases the risk of instability or failure to learn\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* rewards can be _sparse_, thus not provided in every timestep, but only when some goal state is reached\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "* _Glitching or breaking_ the environment: depending on the reward, the agent might exploit _problems_ in the environment\n",
    "    * Example 1: If only the angle is penalized in the inverted pendulum, a constant translational velocity might occur \n",
    "    * Example 2: Bicycle: when the reward is set to the lateral distance to the closest point on the path it should follow, when leaving the path, being normal to the path leads to the best reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1039896-208d-443e-990a-8b6a8a3a2965",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### On-Policy\n",
    "\n",
    "* On-policy algorithms directly optimize the policy, requiring data from the current policy to calculate updates.  \n",
    "* Generally less sample efficient, but more stable. \n",
    "\n",
    "\n",
    "\n",
    "<br> <!-- Add extra space between sections -->\n",
    "\n",
    "### Off-Policy: \n",
    "\n",
    "* Off-policy algorithms learn from data not generated by the current policy.\n",
    "* Typically, data is saved into a buffer and reused by sampling from the buffer, increasing sample efficiency.\n",
    "    * the most recent agent is used to generate the data\n",
    "* The buffer contains tuples $(\\mathbf{s}_t, \\mathbf{a}_t, r_t, \\mathbf{s}_{t+1}, done)$ to learn from.\n",
    "* typically: before learning starts, the buffer is (partly) filled to start learning with diverse, uncorrelated data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee97ae28-e155-4299-9303-03700391f8a9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017112fa-91c2-4ba8-a592-670983bfa088",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Deep-Q-Network (DQN): \n",
    "\n",
    "\n",
    "\n",
    "<!--div style=\"width: 50%; margin: 0 auto; text-align: center;\"-->\n",
    "\n",
    "The expected discounted reward the agent will receive can be expressed by:  \n",
    "$Q_\\pi(s, a) = \\mathbb{E}\\left[r_t+\\gamma r_{t+1}+\\gamma^2 r_{t+2}+\\ldots \\mid s_t=s, a_t=a\\right]$\n",
    "-  _expected_ because the environment and policy might be stochastic\n",
    "- _discounted_ because future reward are weighted with discount factor $\\gamma \\in [0, 1)$\n",
    "- in terminal state: no future rewards\n",
    "* starting in state $\\mathbf{s}$\n",
    "* taking action $a$\n",
    "* following policy $\\pi$\n",
    "The policy $\\pi(s)= \\underset{a \\in A}{\\arg \\max } Q_\\pi(s, a)$ chooses the action that maximizes the expected return\n",
    "\n",
    "\n",
    "<!--Bellman equation for optimal value function:  \n",
    "$ Q^*\\left(s_t, a_t\\right)=\\mathbb{E}\\left[r\\left(s_t, a_t\\right)+\\gamma \\max _{a^{\\prime}} Q^*\\left(s_{t+1}, a^{\\prime}\\right)\\right] $\n",
    "\n",
    "__Q-Learning__: \n",
    "* learn the optimal Q-function $Q^*$, thus the best expected return from given states s and action a. \n",
    "* For a known $Q^*$ the optimal policy is $\\pi^*(\\mathbf{s}) = \\arg \\max  Q^*(\\mathbf{s},a)$ \n",
    "\n",
    "-->\n",
    "\n",
    "__Deep Q Networks (DQN)__: \n",
    "* Concept: Learn the optimal Q-function $Q^*$, thus the best expected return from given states s and action a:  \n",
    "   $ Q^*\\left(s_t, a_t\\right)=\\mathbb{E}\\left[r\\left(s_t, a_t\\right)+\\gamma \\max _{a^{\\prime}} Q^*\\left(s_{t+1}, a^{\\prime}\\right)\\right] $\n",
    "* The Q-function is learned by a neural network $Q(s,a, \\theta)$ with parameters $\\theta$ (weights/biases) of the neural network \n",
    "* temporal difference (TD) loss $\\mathcal{L}(\\theta)=\\left(Q(s, a ; \\theta)-\\left[r+\\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta\\right)\\right]\\right)^2$\n",
    "    * Stability problems arise because both the selection of the next action and it's evaluation through the Q-value depend on $\\theta$\n",
    "    * Solution: target network predicts $y = \\gamma \\max _{a^{\\prime}} Q\\left(s^{\\prime}, a^{\\prime} ; \\theta\\right)$\n",
    "    * Loss is now $\\mathcal{L}(\\theta)=\\left(Q(s, a ; \\theta)-y\\right)^2$, helps stabilizing training\n",
    "    * The target network is updated with lower frequency from the Q-network's parameters, either as _hard_ or _soft_ (Polyak) updates\n",
    "* $\\epsilon$-greedy: with chance of $\\epsilon$ a random action is taken, while with probability of $1-\\epsilon$ the best-known action is chosen\n",
    "    * often $\\epsilon$ is initialized close to 1 and decreased over learning steps\n",
    "    * higher $\\epsilon$ \n",
    "\n",
    "\n",
    "Note:  \n",
    "* DQN works only with a discrete action space\n",
    "* off-policy\n",
    "* See also the paper [Human-level control through deep reinforcement learning](https://doi.org/10.1038/nature14236) and [stable-baselines](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html) parameters.\n",
    "* _double_ Q-learning: in Q-learning overestimation of action values might cause poor performance []()\n",
    "* Depending on the input \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "570cbd3d-30e4-4412-8b4a-b29d1f031e8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "the model: policy:  DQNPolicy(\n",
      "  (q_net): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (q_net_target): QNetwork(\n",
      "    (features_extractor): FlattenExtractor(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    )\n",
      "    (q_net): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=128, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ") \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example DQN code\n",
    "import gymnasium as gym \n",
    "from torch import tensor\n",
    "from stable_baselines3 import DQN\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\") # inverted pendulum on cart\n",
    "\n",
    "# note: here fully connected layers are used. When learning from images/pixel based representations, CNNs are commonly used.\n",
    "# Also RNNs (Recurrent Neural Networks - see Introduction) can be applied. \n",
    "model = DQN.load(\"agent_dqn_cartpole_rewardsimple\", env=env) # previously trained model\n",
    "observation = tensor([1,0,0,0])\n",
    "\n",
    "print(\"the model: policy: \", model.policy, '\\n'*2)\n",
    "# note: the input of q network for the cartpole has 4 dimensions --> state = [x, x_t, phi, phi_t]\n",
    "# The output of the q network has 2 dimensions --> 2 discrete actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a06b1-1ae0-4cf0-8e33-07450b0219f2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Soft Actor-Critic (SAC)\n",
    "\n",
    "    \n",
    "* Just as DQN, SAC is also __off-policy__ and learns __Q-functions__ (both Q-network and target Q-network)\n",
    "\n",
    "* entropy regularization: policy maximizes trade-off between expected return and entropy $\\mathcal{H}$ (exploration)\n",
    "\\begin{equation}\n",
    "\\pi^*=\\arg \\max _\\pi \\sum_t \\mathbb{E}_{\\left(s_t, a_t\\right) \\sim \\pi}\\left[r\\left(s_t, a_t\\right)+\\alpha \\cdot \\mathcal{H}\\left(\\pi\\left(\\cdot \\mid s_t\\right)\\right)\\right]\n",
    "\\end{equation}\n",
    "\n",
    "* Uses actor and critic networks:\n",
    "    * __actor__: determines the action to take according to the policy function\n",
    "    * __critic__: evaluate the action\n",
    "\n",
    "* The stochastic critic outputs a mean and standard deviation, enabling continuous actions\n",
    "* double _clipped_ Q-function: two Q-functions are learned and the $min(Q_{\\theta_1}(\\mathbf{s}, a), Q_{\\theta_2}(\\mathbf{s}, a))$ used to avoid over-estimation of value.\n",
    "\n",
    "\n",
    "* Only supports continuous actions by default. \n",
    "* See also the paper [Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://doi.org/10.48550/arXiv.1801.01290), 2018. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f0acc-5b9e-45ac-aec2-70f5d22acb90",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import SAC\n",
    "env_continous = gym.make(\"Pendulum-v1\", render_mode=\"human\") \n",
    "model_SAC = SAC(\"MlpPolicy\", env_continous, verbose=1)\n",
    "\n",
    "print('SAC actor: ', model_SAC.actor, '\\n'*2)\n",
    "print('*'*70, '\\nSAC critic uses two Q-networks: \\n', model_SAC.critic) # two Q-networks, where the min(Q1, Q2) is used for the update to reduce overestimation of Q-values\n",
    "print('*'*70, '\\nand two target Q-networks: \\n', model_SAC.critic_target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b298e33-1a9b-4735-a6d3-3a5e17d015a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Asynchronous Advantage Actor Critic (A3C)\n",
    "\n",
    "\n",
    "1. Actor-Critic architecture\n",
    "    * __actor__: provides the policy $\\pi(a|s,\\theta)$, choosing the action\n",
    "    * __critic__: estimates the state's value $V (s,\\theta)$\n",
    "    * Seperated loss functions $\\mathcal{L}_{actor} = - \\log{\\pi(a_t| s_t, \\theta) A_t}$ and $\\mathcal{L}_{critic} = \\left(R_t - V(s_t, \\theta_v)\\right)^2$\n",
    "    * _Advantage_ $A_t = R_t - V(s_t)$: $A >0$:  action is better than expected -> increase probability\n",
    "    * Optionally: entropy term for exploration\n",
    "\n",
    "* A3C: multiple environments/threads run in parallel and update a shared model asynchronously\n",
    "* A2C: synchronized version (also parallelized), but waits and applies update from all threads\n",
    "    * A2C is better suited for GPU implementation, A3C for CPU\n",
    "\n",
    "* on-policy, no experience buffer. \n",
    "* supports discrete and continuous actions\n",
    "* In practice actor/critic often share parts of the neural network layers.\n",
    "* see papers [Asynchronous Methods for Deep Reinforcement Learning](https://doi.org/10.48550/arXiv.1602.01783)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb6fb86-29e5-478b-8259-f220fdb29b41",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C\n",
    "model_A2C = A2C(\"MlpPolicy\", env, verbose=1)\n",
    "print('A2C structure: ', model_A2C.policy, '\\n'*2, '*'*70)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d8e87-56f4-4376-815a-1cc82f59e1e2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Proximal Policy Optimization (PPO): \n",
    "    \n",
    "PPO is a policy gradient method, learning on-policy: no buffer is used and the latest data is discarded. \n",
    "With the default Policy Gradient Loss \n",
    "\\begin{equation}\n",
    "\\mathcal{L}^{PG}(\\theta) = \\mathbb{E}_t\\left[log\\pi_\\theta (a_t | \\mathbf{s}_t) \\hat{A}_t \\right]\n",
    "\\end{equation}\n",
    "with Advantage $\\hat{A} = R_t - V(\\mathbf{s}_t)$. \n",
    "* for positive advantages the gradient is positive, thus the action probability is positive\n",
    "* for negative advantages the gradient is negative and the action probability decreases \n",
    "* Trust-region optimization (TRPO) is the basis of PPO by introducing a constraint - but this constraint adds training/implementation overhead\n",
    "\n",
    "In Proximal Policy Optimization (PPO) the loss is \n",
    "\\begin{equation}\n",
    "\\mathcal{L}^{\\text{PPO}}(\\theta) = \\mathbb{E}_t \\left[ \n",
    "\\min\\left( r_t(\\theta) \\hat{A}_t,\\ \n",
    "\\text{clip}\\left(r_t(\\theta),\\ 1 - \\epsilon,\\ 1 + \\epsilon\\right) \\hat{A}_t \\right)\n",
    "\\right]\n",
    "\\end{equation}\n",
    "clipping using the hyperparameter $\\epsilon$ which controls the size of the trust region and the probability ratio $r_t$: how much the policy changed. \n",
    "* Clipping reduces the adaptation of the policy in the update step as the advantage estimation might be noisy\n",
    "\n",
    "In addition to the clipping Loss, entropy is added to promote exploration. \n",
    "\n",
    "\n",
    "* on-policy, discrete and continuous. \n",
    "<!---\n",
    "shared network parts\n",
    "Example PPO: OpenAI Five (Dota)\n",
    "https://github.com/henanmemeda/RL-Adventure-2/blob/master/3.ppo.ipynb\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473fb3b-911e-4396-a137-04f08d1538da",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "model_PPO = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "print('\\n\\nPPO structure: \\n', model_PPO.policy, '\\n'*2, '*'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8560e706-0ef9-4222-939e-58803b95a1df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Libraries\n",
    "\n",
    "\n",
    "Widely used libraries for reinforcement learning are: \n",
    "* [gymnasium](https://gymnasium.farama.org/index.html): a collection of environments, previously developed as [gym](https://www.gymlibrary.dev/index.html).  \n",
    "* [pytorch](https://pytorch.org/): widely used for training of neural networks, also features an rl library. \n",
    "* [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/): A set of reliable implementations of RL algorithms using pytorch at the backend.  \n",
    "* [tensorboard](https://www.tensorflow.org/tensorboard): visualization of the learning process. \n",
    "* [ray](https://www.ray.io/): scaling (RL) tasks on heterogeneous clusters\n",
    "\n",
    "\n",
    "<!--* (Nvidia) [Isaac Lab](https://developer.nvidia.com/isaac/lab): framework for robotics learning built on Isaac Sim. -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217e370-564f-4ed4-b076-e79f2525c9e6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Session Content: \n",
    "\n",
    "    \n",
    "* [3-2_DQN_and_Cart_Pole](3-2_DQN_and_Cart_Pole.ipynb): Run a training of an RL agent on the _cartpole_ using DQN. \n",
    "* [3-3_Custom_Environment_Exudyn](3-3_Custom_Environment_Exudyn.ipynb): Recreation of the _cartpole_ example using a __custom__ gymnasium environment, __vectorized__ (parallel) environments for speedup, and the multibody code Exudyn. __Tensorboard__ is used for visualization while training.   \n",
    "* [3-4_Application_Agents](3-4_Application_Agents.ipynb): Apply the agents (trained in scripts 3-2 and 3-3) to the _cartpole_ and see the influence of different rewards.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e65159-0b57-4142-afa7-da2dd2fbfc4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Let's see how we can apply these algorithms!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "toc": {
   "base_numbering": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
